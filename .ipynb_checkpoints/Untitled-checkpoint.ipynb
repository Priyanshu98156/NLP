{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "6a99ac97-c0d8-4991-829c-1b815991e96d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "import spacy "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "2fca4702-2a36-4da5-8259-c85a075c3eef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original : ['Natural', 'Language', 'Processing', 'is', 'fun', 'and', 'powerful', '!']\n",
      "Filtered:  ['Natural', 'Language', 'Processing', 'fun', 'powerful', '!']\n"
     ]
    }
   ],
   "source": [
    "text = \"Natural Language Processing is fun and powerful!\"\n",
    "tokens = word_tokenize(text)\n",
    "stop_words = set(stopwords.words('english'))\n",
    "filtered_tokens = [w for w in tokens if w.lower() not in stop_words]\n",
    "\n",
    "\n",
    "print(\"Original :\", tokens)\n",
    "print(\"Filtered: \", filtered_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f711be1-defd-43c1-a915-5adf9981fc51",
   "metadata": {},
   "source": [
    "<h3>Lemmatization and stemming</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "f3c9ea9f-e2dd-4cdf-9e1c-df82c56ac0d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stemming :\n",
      "running -> run\n",
      "playing -> play\n",
      "better -> better\n",
      "studies -> studi\n",
      "\n",
      "Lemmatization : \n",
      "running -> run\n",
      "playing -> play\n",
      "better -> better\n",
      "studies -> study\n"
     ]
    }
   ],
   "source": [
    "stemmer    = PorterStemmer()\n",
    "lemmatizer =  WordNetLemmatizer()\n",
    "words = [\"running\", \"playing\", \"better\", \"studies\"]\n",
    "print(\"Stemming :\")\n",
    "for word in words:\n",
    "    print(word, \"->\" , stemmer.stem(word))\n",
    "\n",
    "print(\"\\nLemmatization : \")\n",
    "for word in words:\n",
    "    print(word, \"->\", lemmatizer.lemmatize(word, pos= 'v')) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b31677e8-3f40-4b13-81ce-d480ab285f69",
   "metadata": {},
   "source": [
    "lemmatizer.lemmatize(word, pos= 'v') \n",
    "You're telling the lemmatizer:\n",
    "\n",
    "Hey, treat this word as a verb when lemmatizing.\"\n",
    "'n'\t= noun\n",
    "'v'\t= verb\n",
    "'a'\t= adjective\n",
    "'r'\t= adverb\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "1ad4384c-38e7-4f43-8b37-cc796cf816ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Apple        | POS: PROPN      | Lemma: Apple\n",
      "is           | POS: AUX        | Lemma: be\n",
      "looking      | POS: VERB       | Lemma: look\n",
      "at           | POS: ADP        | Lemma: at\n",
      "buying       | POS: VERB       | Lemma: buy\n",
      "U.K.         | POS: PROPN      | Lemma: U.K.\n",
      "startup      | POS: VERB       | Lemma: startup\n",
      "for          | POS: ADP        | Lemma: for\n",
      "$            | POS: SYM        | Lemma: $\n",
      "1            | POS: NUM        | Lemma: 1\n",
      "billon       | POS: NOUN       | Lemma: billon\n",
      ".            | POS: PUNCT      | Lemma: .\n"
     ]
    }
   ],
   "source": [
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "doc = nlp(\"Apple is looking at buying U.K. startup for $1 billon.\")\n",
    "\n",
    "for token in doc: \n",
    "    print(f\"{token.text:<12} | POS: {token.pos_:<10} | Lemma: {token.lemma_}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "8e342932-50a4-4e8c-85a8-810ad798cfd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#loading the news article\n",
    "path = \"D:/NLP/Day1/newsArticle.txt\"\n",
    "with open(path, 'r', encoding='utf-8') as f:\n",
    "    data = f.read()\n",
    "tokens = word_tokenize(data)\n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "filtered_news_article = [word for word in tokens if word.lower() not in stop_words]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "5432f61e-cd0a-454f-902a-fecf55577260",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stemming:\n",
      "India -> india\n",
      "’ -> ’\n",
      "home -> home\n",
      "season -> season\n",
      ": -> :\n",
      "Kolkata -> kolkata\n",
      ", -> ,\n",
      "Delhi -> delhi\n",
      "swap -> swap\n",
      "Tests -> test\n",
      "West -> west\n",
      "Indies -> indi\n",
      "South -> south\n",
      "Africa -> africa\n",
      "understood -> understood\n",
      "\n",
      " Lemmatization:\n",
      "India -> India\n",
      "’ -> ’\n",
      "home -> home\n",
      "season -> season\n",
      ": -> :\n",
      "Kolkata -> Kolkata\n",
      ", -> ,\n",
      "Delhi -> Delhi\n",
      "swap -> swap\n",
      "Tests -> Tests\n",
      "West -> West\n",
      "Indies -> Indies\n",
      "South -> South\n",
      "Africa -> Africa\n",
      "understood -> understand\n",
      "\n",
      " spaCy POS Tagging + Lemmas:\n",
      "India        | POS: PROPN      | Lemma: India\n",
      "’            | POS: PUNCT      | Lemma: '\n",
      "home         | POS: NOUN       | Lemma: home\n",
      "season       | POS: NOUN       | Lemma: season\n",
      ":            | POS: PUNCT      | Lemma: :\n",
      "Kolkata      | POS: PROPN      | Lemma: Kolkata\n",
      ",            | POS: PUNCT      | Lemma: ,\n",
      "Delhi        | POS: PROPN      | Lemma: Delhi\n",
      "swap         | POS: VERB       | Lemma: swap\n",
      "Tests        | POS: PROPN      | Lemma: Tests\n",
      "West         | POS: PROPN      | Lemma: West\n",
      "Indies       | POS: PROPN      | Lemma: Indies\n",
      "South        | POS: PROPN      | Lemma: South\n",
      "Africa       | POS: PROPN      | Lemma: Africa\n",
      "understood   | POS: VERB       | Lemma: understand\n",
      "air          | POS: NOUN       | Lemma: air\n",
      "pollution    | POS: NOUN       | Lemma: pollution\n",
      "levels       | POS: NOUN       | Lemma: level\n",
      "mid          | POS: PROPN      | Lemma: mid\n",
      "-            | POS: PROPN      | Lemma: -\n",
      "November     | POS: PROPN      | Lemma: November\n",
      "National     | POS: PROPN      | Lemma: National\n",
      "Capital      | POS: PROPN      | Lemma: Capital\n",
      "Region       | POS: PROPN      | Lemma: Region\n",
      "become       | POS: VERB       | Lemma: become\n",
      "health       | POS: NOUN       | Lemma: health\n",
      "hazard       | POS: NOUN       | Lemma: hazard\n",
      "like         | POS: SCONJ      | Lemma: like\n",
      "happened     | POS: VERB       | Lemma: happen\n",
      "Test         | POS: PROPN      | Lemma: Test\n",
      "match        | POS: NOUN       | Lemma: match\n",
      "Sri          | POS: PROPN      | Lemma: Sri\n"
     ]
    }
   ],
   "source": [
    "# Display Stemming\n",
    "print(\"Stemming:\")\n",
    "for word in filtered_news_article[:15]:\n",
    "    print(f\"{word} -> {stemmer.stem(word)}\")\n",
    "\n",
    "# Display Lemmatization\n",
    "print(\"\\n Lemmatization:\")\n",
    "for word in filtered_news_article[:15]:\n",
    "    print(f\"{word} -> {lemmatizer.lemmatize(word, pos='v')}\")\n",
    "\n",
    "# spaCy POS + Lemma\n",
    "print(\"\\n spaCy POS Tagging + Lemmas:\")\n",
    "doc = nlp(\" \".join(filtered_news_article[:30]))  # Use a subset for clean output\n",
    "for token in doc:\n",
    "    print(f\"{token.text:<12} | POS: {token.pos_:<10} | Lemma: {token.lemma_}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "837b558c-02fb-4836-8dcb-1c7769d87a3e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
